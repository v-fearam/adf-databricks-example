{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39623ee2-a9fa-4926-be7f-5bc2ef634586",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get notebook parameter from Azure pipeline\n",
    "dbutils.widgets.text(\"_pipeline_run_id\",\"123-123\")\n",
    "dbutils.widgets.text(\"_filename\",\"nybabynames.csv\")\n",
    "_pipeline_run_id = dbutils.widgets.get(\"_pipeline_run_id\")\n",
    "_filename = dbutils.widgets.get(\"_filename\")\n",
    "print (_pipeline_run_id)\n",
    "print(_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c744dc0e-ec9a-44bf-baf1-e3e76002e148",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure my account key and account name so Databricks can access the Data Lake\n",
    "accountName = dbutils.secrets.get(\"dataLakeScope\",\"accountName\")\n",
    "accountKey = dbutils.secrets.get(\"dataLakeScope\",\"accountKey\")\n",
    "sparkProperty = f'fs.azure.account.key.{accountName}.dfs.core.windows.net'\n",
    "spark.conf.set(sparkProperty,accountKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a815703-e57b-4825-97a4-9af30089ab1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the location of my files\n",
    "bronzeSource = f'abfss://bronze@{accountName}.dfs.core.windows.net/{_filename}'\n",
    "silverTarget = f'abfss://silver@{accountName}.dfs.core.windows.net/nybabynames'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31e7804f-3ac5-48ef-b09a-4b7d5086323f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read data from Data Lake\n",
    "gridDataDf = spark.read.option(\"inferSchema\", \"true\").csv(path= bronzeSource, header=True)\n",
    "\n",
    "display(gridDataDf.printSchema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40878786-329f-407d-b20b-5af5ac7052ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from  pyspark.sql.functions import *\n",
    "\n",
    "# Add audit column to the data frame \n",
    "\n",
    "processing_date = date_trunc(\"second\",current_timestamp())\n",
    "\n",
    "# 1. Rename count column \n",
    "# 2. Adding current time to process this data set\n",
    "# 3. Adding pipepeline run id from ADF\n",
    "# 4. The landig file name. This is useful for debugging prurpose\n",
    "# 5. Modification date. This help identified order of data when the dataset doesn't have a modification date\n",
    "gridDataDf = gridDataDf.withColumnRenamed(\"name_count\", \"count\") \\\n",
    "                       .withColumn(\"_processing_date\", processing_date) \\\n",
    "                       .withColumn(\"_pipeline_run_id\", lit(_pipeline_run_id)) \\\n",
    "                       .withColumn(\"_input_filename\", input_file_name()) \\\n",
    "                       .withColumn(\"_input_file_modification_date\", col(\"_metadata.file_modification_time\"))\n",
    "\n",
    "display(gridDataDf.printSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "320a8a77-09a3-4662-8b97-cad8d68601ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from  pyspark.sql.functions import *\n",
    "from  pyspark.sql import *\n",
    "\n",
    "#  Data Quality\n",
    "gridCleanDF = gridDataDf.filter(\"year IS NOT NULL AND first_name IS NOT NULL AND county IS NOT NULL AND sex IS NOT NULL AND count IS NOT NULL AND count > 0\")\n",
    "\n",
    "\n",
    "# Data Duplication \n",
    "gridDataWindowSpec = Window.partitionBy(\"year\",\"first_name\",\"county\",\"sex\").orderBy(col(\"_input_file_modification_date\").desc(),\"count\")\n",
    "findLatest = gridCleanDF.withColumn(\"row_number\",row_number().over(gridDataWindowSpec)).filter(\"row_number == 1\").drop(\"row_number\")\n",
    "\n",
    "gridDataDf = findLatest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaa7f5e8-f4ff-4dfa-a67d-e9cbbdbb97fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "# check if the silver contain the delta table\n",
    "if(DeltaTable.isDeltaTable(spark, silverTarget)): \n",
    "\n",
    "    # If yes, merge data with the existing delta table\n",
    "    DeltaTable.forPath(spark, silverTarget).alias(\"target\").merge(\n",
    "        source = gridDataDf.alias(\"src\"),\n",
    "        condition = \"target.year = src.year and target.first_name = src.first_name and target.county = src.county and target.sex = src.sex\"\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "else:\n",
    "\n",
    "    # If no, save the file to silver\n",
    "    gridDataDf.write.mode(\"overwrite\").format(\"delta\").save(silverTarget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d279b098-c8b7-4019-b382-7b403e1aa787",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create the schema and table, if required\n",
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS nybabynames\")\n",
    "spark.sql(f\"CREATE EXTERNAL TABLE IF NOT EXISTS nybabynames.NewYorkBabyNames USING delta LOCATION '{silverTarget}'\")\n",
    "\n",
    "# Note: Using spark.sql because we can use f-string to retrieve the silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f40a2c8d-5f52-484c-8255-c573ff15907d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "DESCRIBE EXTENDED nybabynames.NewYorkBabyNames\n",
    "\n",
    "-- Location: stored in the storage account\n",
    "-- Provider (format): Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "560467df-80ed-49d6-82bb-48fec7f08957",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- Show the transaction log on the delta version\n",
    "\n",
    "SELECT version, operationMetrics, operationMetrics.numOutputRows, operationMetrics.numTargetRowsInserted, operationMetrics.numTargetRowsUpdated, operationMetrics.numTargetRowsDeleted\n",
    "FROM (DESCRIBE HISTORY nybabynames.NewYorkBabyNames)\n",
    "\n",
    "-- if you execute Cmd 7 again, a new version is created with no data changes (numOutputRows = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9898ce3-e620-4ca3-aab1-b698ac712218",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- Check your result for testing. Do not do this in production!\n",
    "SELECT *\n",
    "FROM nybabynames.NewYorkBabyNames\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2598784971005238,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "mynotebook",
   "widgets": {
    "_filename": {
     "currentValue": "nybabynames-20-05-2024.csv",
     "nuid": "892949d2-02a9-4717-b7a2-718666dbae2b",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "nybabynames.csv",
      "label": null,
      "name": "_filename",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "_pipeline_run_id": {
     "currentValue": "123-123",
     "nuid": "071d5985-d32d-4455-a05f-e5dace19ed00",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "123-123",
      "label": null,
      "name": "_pipeline_run_id",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
